{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1Z4wVB_kyNiPMq6yagm0BHHHm9fOnBeer","authorship_tag":"ABX9TyMapqclSuMFmpE16d8lNAA0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"bf75600774ac43be82fa67fe166f2c51":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1efc98e6188942d2b1a85ee90362332d","IPY_MODEL_de20d30246904e8686ce94087411c929","IPY_MODEL_ce5364ccc69f4913bf788fc59e23fb54"],"layout":"IPY_MODEL_cbc98aa1372f4dd4a0ea0896c5e24ec3"}},"1efc98e6188942d2b1a85ee90362332d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ea5ec3a9c2e456e9912cdfa5aef730c","placeholder":"​","style":"IPY_MODEL_d69b90f447594cffa1b1b40302906c24","value":"tokenizer_config.json: 100%"}},"de20d30246904e8686ce94087411c929":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d546b7d5f84c464f888a3572cb4668e0","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7c0f78f524ca47f1afe22c8d0643a2f8","value":28}},"ce5364ccc69f4913bf788fc59e23fb54":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_68149b7de78e437bbefd700db5cefd21","placeholder":"​","style":"IPY_MODEL_24bd4e9036d34ac7b56d41352a8cd661","value":" 28.0/28.0 [00:00&lt;00:00, 1.30kB/s]"}},"cbc98aa1372f4dd4a0ea0896c5e24ec3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ea5ec3a9c2e456e9912cdfa5aef730c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d69b90f447594cffa1b1b40302906c24":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d546b7d5f84c464f888a3572cb4668e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c0f78f524ca47f1afe22c8d0643a2f8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"68149b7de78e437bbefd700db5cefd21":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24bd4e9036d34ac7b56d41352a8cd661":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ecd8028bd7e846879ed5c35067798f04":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2f0378fe454f4c4986a22803122b4d16","IPY_MODEL_7435fdec3dca4b5794a59c4cb3968822","IPY_MODEL_87a579d0e13b4f1db023f714acc4ed9a"],"layout":"IPY_MODEL_c341c528de42499394049a2506d6face"}},"2f0378fe454f4c4986a22803122b4d16":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_710de8e66ab844f3b0818322bce3e3e2","placeholder":"​","style":"IPY_MODEL_95d21f722e7b487cb1adf7a91f36193c","value":"vocab.txt: 100%"}},"7435fdec3dca4b5794a59c4cb3968822":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a51e815753a34e7eb8f242eca703164f","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8997347eb9d642c2b538ebf4c90279d1","value":231508}},"87a579d0e13b4f1db023f714acc4ed9a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a21cc4ffd6c0456598edf201fc15d610","placeholder":"​","style":"IPY_MODEL_c7486b6ace0e4829a81c91d8840659ea","value":" 232k/232k [00:00&lt;00:00, 3.96MB/s]"}},"c341c528de42499394049a2506d6face":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"710de8e66ab844f3b0818322bce3e3e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95d21f722e7b487cb1adf7a91f36193c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a51e815753a34e7eb8f242eca703164f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8997347eb9d642c2b538ebf4c90279d1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a21cc4ffd6c0456598edf201fc15d610":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7486b6ace0e4829a81c91d8840659ea":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"14c027f7c1544b8aac97a9337d5d4f0e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f7bb78d83fdb45e699a6e1bd0dd1e3ab","IPY_MODEL_3157b2d73973476b9d1755127c46a855","IPY_MODEL_1566046027444f49a10704f218be460e"],"layout":"IPY_MODEL_5f68dfa2e8e748468bf92d5278459d17"}},"f7bb78d83fdb45e699a6e1bd0dd1e3ab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3c7401ae09a4dc084365706fbd0da83","placeholder":"​","style":"IPY_MODEL_58f792bf1b80430392cb62fd79ef2557","value":"tokenizer.json: 100%"}},"3157b2d73973476b9d1755127c46a855":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b1d41bf4b804201893f0714b913e426","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7ec0d2dc64e54201bad9c29c53c216e8","value":466062}},"1566046027444f49a10704f218be460e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_09232fe8c1394b0ab569840205c3a7a8","placeholder":"​","style":"IPY_MODEL_6661f9db83634a12bee69a17bd1ffa10","value":" 466k/466k [00:00&lt;00:00, 9.15MB/s]"}},"5f68dfa2e8e748468bf92d5278459d17":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3c7401ae09a4dc084365706fbd0da83":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58f792bf1b80430392cb62fd79ef2557":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5b1d41bf4b804201893f0714b913e426":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ec0d2dc64e54201bad9c29c53c216e8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"09232fe8c1394b0ab569840205c3a7a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6661f9db83634a12bee69a17bd1ffa10":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"836e6a73326b40648dd3c6675f2da0ab":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_82e7634237394b5e8407b2ffe3e9acf0","IPY_MODEL_168c756a541441f184d15fbb01ad9f0e","IPY_MODEL_5e9310ff2bb044d7ac4b641f9ba3889f"],"layout":"IPY_MODEL_b0567ae385d44facbec281b7ef396664"}},"82e7634237394b5e8407b2ffe3e9acf0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_33732bf2d5a842ee9acb4edbfc8c028b","placeholder":"​","style":"IPY_MODEL_5b5b1ae74c1447b3b690ebfa49310665","value":"config.json: 100%"}},"168c756a541441f184d15fbb01ad9f0e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9692d262c5b34da3931c0cec4d3d1938","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f5e6832d2eb440f69730751acc1c62b5","value":570}},"5e9310ff2bb044d7ac4b641f9ba3889f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3849910836e7449abda59290292eb3a8","placeholder":"​","style":"IPY_MODEL_3c403708d0d442d0ac3b75353a11fc87","value":" 570/570 [00:00&lt;00:00, 36.7kB/s]"}},"b0567ae385d44facbec281b7ef396664":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33732bf2d5a842ee9acb4edbfc8c028b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b5b1ae74c1447b3b690ebfa49310665":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9692d262c5b34da3931c0cec4d3d1938":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5e6832d2eb440f69730751acc1c62b5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3849910836e7449abda59290292eb3a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c403708d0d442d0ac3b75353a11fc87":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eb1de445fd654603ba2a9db747c80608":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_32367f08c8834707a76b0850e3f40d3f","IPY_MODEL_a7a531857ce5493eaaa66486f826fed4","IPY_MODEL_a0eb245c8c5b4fdfa3e02ed115064021"],"layout":"IPY_MODEL_9b7b1bcb1d9442d78bdc22c133fe9de9"}},"32367f08c8834707a76b0850e3f40d3f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f3740b1935043a6ae98080d022a37b6","placeholder":"​","style":"IPY_MODEL_0c301d2060e74ba880c5437f1d14a8f5","value":"model.safetensors: 100%"}},"a7a531857ce5493eaaa66486f826fed4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7546d0879f934a69abf948fb593d1d32","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_692a96737d9247399447146b591e1add","value":440449768}},"a0eb245c8c5b4fdfa3e02ed115064021":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_45b61afdaa854c3da144e518aca3dede","placeholder":"​","style":"IPY_MODEL_3cbca69cc3984d5fad09374cf7d442fe","value":" 440M/440M [00:02&lt;00:00, 159MB/s]"}},"9b7b1bcb1d9442d78bdc22c133fe9de9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f3740b1935043a6ae98080d022a37b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c301d2060e74ba880c5437f1d14a8f5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7546d0879f934a69abf948fb593d1d32":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"692a96737d9247399447146b591e1add":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"45b61afdaa854c3da144e518aca3dede":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3cbca69cc3984d5fad09374cf7d442fe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["#Sentiment analysis of customer comments with Models Based on learning transfer (BERT Language Model)\n","\n","The purpose of this project is to analyze the sentiments of IMDB website users' comments with the help of models based on learning transfer. In this project, we have used the BERT Language model to develop a model that can analyze the sentiments in the comments of IMDB website users and determine whether a comment is positive or negative.\n","\n","The link to the data page on the Keggle website: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n","\n","##BERT (Bidirectional Encoder Representations from Transformers) Language Model\n","\n","BERT is an open source machine learning framework for natural language processing (NLP). BERT is designed to help computers understand the meaning of ambiguous language in text by using surrounding text to establish context. The BERT framework was pre-trained using text from Wikipedia and can be fine-tuned with question and answer datasets.\n","\n","BERT, which stands for Bidirectional Encoder Representations from Transformers, is based on Transformers, a deep learning model in which every output element is connected to every input element, and the weightings between them are dynamically calculated based upon their connection. (In NLP, this process is called attention.)\n","\n","Historically, language models could only read text input sequentially -- either left-to-right or right-to-left -- but couldn't do both at the same time. BERT is different because it is designed to read in both directions at once. This capability, enabled by the introduction of Transformers, is known as bidirectionality.\n","\n","Using this bidirectional capability, BERT is pre-trained on two different, but related, NLP tasks: Masked Language Modeling and Next Sentence Prediction.\n","\n","The objective of Masked Language Model (MLM) training is to hide a word in a sentence and then have the program predict what word has been hidden (masked) based on the hidden word's context. The objective of Next Sentence Prediction training is to have the program predict whether two given sentences have a logical, sequential connection or whether their relationship is simply random.\n","\n","##Import Library and dataset\n","\n","1. **Pandas (import pandas as pd):** Pandas is a powerful data manipulation and analysis library for Python. It provides data structures like DataFrame, which is a two-dimensional table, and tools for data cleaning, exploration, and manipulation. Pandas is used to read the dataset from a CSV file, perform data manipulations (e.g., splitting the dataset), and create DataFrames for easy handling of tabular data.\n","\n","2. **scikit-learn (from sklearn.model_selection import train_test_split):** Scikit-learn is a machine learning library that provides simple and efficient tools for data analysis and modeling. The train_test_split function is used to split the dataset into training and testing sets. The train_test_split function is applied to divide the dataset into portions for training and evaluating the model.\n","\n","3. **from sklearn.preprocessing import LabelEncoder:** The LabelEncoder is a utility class provided by scikit-learn that helps encode categorical labels (strings or integers) into numerical values. It assigns a unique integer to each unique label. In the provided code, LabelEncoder is used to convert the 'sentiment' column in the dataset from string labels ('positive' and 'negative') to numeric labels (0 and 1). This is necessary for training the machine learning model, which typically expects numerical labels.\n","\n","4. **from sklearn.metrics import accuracy_score, classification_report:**\n","\"accuracy_score\" Computes the accuracy of the model predictions by comparing them with the true labels. It is a common metric for classification tasks. \"classification_report\" Generates a text report showing the main classification metrics, including precision, recall, and F1-score, for each class. After making predictions on the test set, these metrics are used to evaluate the performance of the model.\n","\n","5. **Transformers (from transformers import BertTokenizer, BertForSequenceClassification, AdamW):** Transformers is a library by Hugging Face that provides pre-trained models for natural language processing (NLP) tasks. It includes tools for working with popular transformer-based models like BERT, GPT, and others.  In this code, the BertTokenizer is used to tokenize the input text, BertForSequenceClassification is the pre-trained BERT model for sequence classification, and AdamW is an optimizer for training the model.\n","\n","6. **PyTorch (import torch):** PyTorch is an open-source machine learning library used for tasks such as deep learning and neural network training. It provides a flexible and dynamic computational graph, making it popular for research and development. PyTorch is used for various tasks, including creating tensors, defining and training neural networks, and moving data between CPU and GPU.\n","\n","7. **Torch DataLoader (from torch.utils.data import DataLoader, TensorDataset, random_split):** The DataLoader class from PyTorch is used to load batches of data efficiently during model training. TensorDataset is a PyTorch dataset wrapper for tensors, and random_split is used to split the dataset into training and validation sets. These components are used to prepare the data in a format suitable for training the BERT model.\n","\n","8. **tqdm (from tqdm import tqdm):** tqdm is a library for adding progress bars to loops in Python. It provides a visual representation of the progress of tasks, which is particularly helpful for time-consuming operations. tqdm is used to create progress bars during the training and evaluation loops to monitor the progress of the model.\n","\n","These libraries collectively facilitate the implementation of a sentiment analysis model using transfer learning with BERT. They cover data manipulation, model training, and evaluation, as well as providing pre-trained models and tools for working with natural language processing tasks."],"metadata":{"id":"Q9IgGCcNlGQN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"72vG6i39Y4wa"},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import accuracy_score, classification_report\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset, random_split\n","from tqdm import tqdm"]},{"cell_type":"markdown","source":["###import dataset"],"metadata":{"id":"9vyQji5NsyBU"}},{"cell_type":"code","source":["df = pd.read_csv('/content/drive/MyDrive/ml/Sentiment-analysis-of-customer-comments/IMDB Dataset.csv')"],"metadata":{"id":"VAvDE7PSZhLA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###information about dataset"],"metadata":{"id":"6MCA8mZls0sS"}},{"cell_type":"code","source":["df.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a_3sFZqUZmqf","executionInfo":{"status":"ok","timestamp":1703998870099,"user_tz":-210,"elapsed":795,"user":{"displayName":"Masoud Gaming","userId":"01529317132170938105"}},"outputId":"708a97ec-d797-468e-b734-646b382a55ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 50000 entries, 0 to 49999\n","Data columns (total 2 columns):\n"," #   Column     Non-Null Count  Dtype \n","---  ------     --------------  ----- \n"," 0   review     50000 non-null  object\n"," 1   sentiment  50000 non-null  object\n","dtypes: object(2)\n","memory usage: 781.4+ KB\n"]}]},{"cell_type":"markdown","source":["##Data Preparation\n","the code is using scikit-learn's LabelEncoder to transform the categorical labels in the 'sentiment' column of the DataFrame (df) into numeric labels. This is a common preprocessing step when working with machine learning models, as many algorithms expect numerical input for the target variable.\n","\n","Here's a breakdown of what each line does:\n","\n","1. **label_encoder = LabelEncoder():** Creates an instance of the LabelEncoder class.\n","\n","2. **df['label'] = label_encoder.fit_transform(df['sentiment']):** The fit_transform method of LabelEncoder is applied to the 'sentiment' column in the DataFrame (df). This method fits the encoder to the unique labels in the 'sentiment' column and transforms them into numerical values. The transformed labels are then assigned to a new column named 'label' in the DataFrame. For example, if the 'sentiment' column originally had values like 'positive' and 'negative', the fit_transform operation would map 'positive' to one numerical value (e.g., 1) and 'negative' to another (e.g., 0). The resulting DataFrame would then have a new column 'label' containing these numeric representations.\n","\n","This transformation is essential when training machine learning models because most algorithms require numerical labels. In the context of sentiment analysis, it enables the model to learn the patterns associated with positive and negative sentiments during the training process. After training, the model can then make predictions on new data, providing numeric labels representing the predicted sentiments.\n"],"metadata":{"id":"-yNcWmjttW6b"}},{"cell_type":"code","source":["label_encoder = LabelEncoder()\n","df['label'] = label_encoder.fit_transform(df['sentiment'])"],"metadata":{"id":"z3gurEUlb6QR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###split data\n","The bottom code is using scikit-learn's train_test_split function to split the dataset into training and testing sets. This is a common step in machine learning to evaluate the performance of the model on unseen data. Here's a breakdown of the code:\n","\n","1. **df['review'].values:** Extracts the values from the 'review' column of the DataFrame (df). This column contains the user comments or reviews about movies.\n","\n","2. **df['label'].values:** Extracts the values from the 'label' column of the DataFrame (df). This column was created in a previous step using LabelEncoder to convert the categorical sentiment labels ('positive' and 'negative') into numerical labels.\n","\n","**train_test_split Function:** The first parameter is the array of features (user reviews), and the second parameter is the array of labels (numerical sentiments).\n","1. test_size=0.2: Specifies that 20% of the data should be reserved for the test set, and the remaining 80% will be used for training.\n","2. random_state=42: Sets a seed for the random number generator to ensure reproducibility. The same seed will always produce the same split.\n","\n","**Return Values:** Four sets of data are returned:\n","1. train_texts: The training set of user reviews.\n","2. test_texts: The test set of user reviews.\n","3. train_labels: The corresponding labels (sentiments) for the training set.\n","4. test_labels: The corresponding labels (sentiments) for the test set.\n","\n","So, after running this code, you have separate arrays (train_texts, test_texts, train_labels, test_labels) that represent the training and testing sets of reviews and their corresponding sentiment labels. These sets are then used for training and evaluating the machine learning model."],"metadata":{"id":"hi4_bttvuULR"}},{"cell_type":"code","source":["train_texts, test_texts, train_labels, test_labels = train_test_split(\n","    df['review'].values,\n","    df['label'].values,\n","    test_size=0.2,\n","    random_state=42\n",")"],"metadata":{"id":"aYMewZirZpMv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Create Model\n","\n","In bottom code, the project is utilizing the Hugging Face transformers library to work with a pre-trained BERT (Bidirectional Encoder Representations from Transformers) model for sentiment analysis. Let's break down each line:\n","\n","1. **tokenizer = BertTokenizer.from_pretrained('bert-base-uncased'):** Initializes a BERT tokenizer with the vocabulary and settings of the 'bert-base-uncased' pre-trained model. The tokenizer is responsible for converting text into tokens that can be understood by the BERT model. Later in the code, tokenizer will be used to tokenize user reviews before feeding them into the BERT model.\n","\n","2. **model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2):** Initializes a BERT model for sequence classification tasks. In this case, the sentiment analysis task involves classifying user reviews into two classes: positive and negative. The model is the core BERT model used for processing sequences (user reviews) and making predictions. It is configured for a binary classification task with num_labels=2, where the two labels represent positive and negative sentiments.\n","\n","In summary, these lines of code set up the BERT tokenizer and model for sentiment analysis. The tokenizer will be used to preprocess the text data, and the model is configured for binary sequence classification, making it suitable for the sentiment analysis task with two classes. These pre-trained components are part of the transfer learning approach, where a model pre-trained on a large corpus is fine-tuned on a specific task (sentiment analysis in this case).\n","\n","####Explain a for warning:\n","**warning:** Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.):\n","\n","The message you're seeing is a warning, not an error. It's informing you that some of the weights in the BertForSequenceClassification model were not initialized from the pre-trained checkpoint (in this case, 'bert-base-uncased'). Specifically, the weights associated with the classifier layer (named 'classifier') are being newly initialized.\n","\n","The reason for this is that BertForSequenceClassification is a pre-trained BERT model that has a classification head (classifier) added on top. When you load the model using from_pretrained('bert-base-uncased', num_labels=2), it initializes the pre-trained BERT weights and adds a new classifier for binary sequence classification with two labels.\n","\n","The warning suggests that if you intend to use this model for predictions or inference, you should consider training it on a downstream task. Fine-tuning on a specific task can help adapt the model to the characteristics of your data and improve performance.\n","\n","If you're using the model for a specific task like sentiment analysis with your own dataset, you would typically proceed with fine-tuning the model on your dataset. Fine-tuning involves training the model on your task-specific dataset for a few additional epochs to adjust its weights to your specific data distribution and task requirements.\n","\n","If you don't plan to fine-tune the model and are just using it for inference on sentiment analysis or a similar task, you can still proceed with using the model. The warning is more of a suggestion for optimal performance rather than a strict requirement."],"metadata":{"id":"jFmYEtOCwdsM"}},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232,"referenced_widgets":["bf75600774ac43be82fa67fe166f2c51","1efc98e6188942d2b1a85ee90362332d","de20d30246904e8686ce94087411c929","ce5364ccc69f4913bf788fc59e23fb54","cbc98aa1372f4dd4a0ea0896c5e24ec3","1ea5ec3a9c2e456e9912cdfa5aef730c","d69b90f447594cffa1b1b40302906c24","d546b7d5f84c464f888a3572cb4668e0","7c0f78f524ca47f1afe22c8d0643a2f8","68149b7de78e437bbefd700db5cefd21","24bd4e9036d34ac7b56d41352a8cd661","ecd8028bd7e846879ed5c35067798f04","2f0378fe454f4c4986a22803122b4d16","7435fdec3dca4b5794a59c4cb3968822","87a579d0e13b4f1db023f714acc4ed9a","c341c528de42499394049a2506d6face","710de8e66ab844f3b0818322bce3e3e2","95d21f722e7b487cb1adf7a91f36193c","a51e815753a34e7eb8f242eca703164f","8997347eb9d642c2b538ebf4c90279d1","a21cc4ffd6c0456598edf201fc15d610","c7486b6ace0e4829a81c91d8840659ea","14c027f7c1544b8aac97a9337d5d4f0e","f7bb78d83fdb45e699a6e1bd0dd1e3ab","3157b2d73973476b9d1755127c46a855","1566046027444f49a10704f218be460e","5f68dfa2e8e748468bf92d5278459d17","e3c7401ae09a4dc084365706fbd0da83","58f792bf1b80430392cb62fd79ef2557","5b1d41bf4b804201893f0714b913e426","7ec0d2dc64e54201bad9c29c53c216e8","09232fe8c1394b0ab569840205c3a7a8","6661f9db83634a12bee69a17bd1ffa10","836e6a73326b40648dd3c6675f2da0ab","82e7634237394b5e8407b2ffe3e9acf0","168c756a541441f184d15fbb01ad9f0e","5e9310ff2bb044d7ac4b641f9ba3889f","b0567ae385d44facbec281b7ef396664","33732bf2d5a842ee9acb4edbfc8c028b","5b5b1ae74c1447b3b690ebfa49310665","9692d262c5b34da3931c0cec4d3d1938","f5e6832d2eb440f69730751acc1c62b5","3849910836e7449abda59290292eb3a8","3c403708d0d442d0ac3b75353a11fc87","eb1de445fd654603ba2a9db747c80608","32367f08c8834707a76b0850e3f40d3f","a7a531857ce5493eaaa66486f826fed4","a0eb245c8c5b4fdfa3e02ed115064021","9b7b1bcb1d9442d78bdc22c133fe9de9","2f3740b1935043a6ae98080d022a37b6","0c301d2060e74ba880c5437f1d14a8f5","7546d0879f934a69abf948fb593d1d32","692a96737d9247399447146b591e1add","45b61afdaa854c3da144e518aca3dede","3cbca69cc3984d5fad09374cf7d442fe"]},"id":"AuKtWI8hZu6P","executionInfo":{"status":"ok","timestamp":1703998915980,"user_tz":-210,"elapsed":6211,"user":{"displayName":"Masoud Gaming","userId":"01529317132170938105"}},"outputId":"4d6e3144-1bcc-48b5-95e1-c4d11deb2d80"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf75600774ac43be82fa67fe166f2c51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecd8028bd7e846879ed5c35067798f04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14c027f7c1544b8aac97a9337d5d4f0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"836e6a73326b40648dd3c6675f2da0ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb1de445fd654603ba2a9db747c80608"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"markdown","source":["###Encoding data\n","\n","In bottom code, the project is using the previously initialized BERT tokenizer (tokenizer) to encode the text data (user reviews) for both the training and testing sets. Let's break down each line:\n","\n","1. **train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=128, return_tensors='pt'):** Tokenizes and encodes the text data in the training set using the BERT tokenizer.\n"," * Parameters:\n","    1. train_texts.tolist(): Converts the training set of text data (user reviews) to a list.\n","    2. truncation=True: Truncates the sequences to a specified maximum length (max_length) if they exceed it.\n","    3. padding=True: Adds padding to sequences that are shorter than the specified maximum length (max_length).\n","    4. max_length=128: Specifies the maximum length of the tokenized sequences.\n","    5. return_tensors='pt': Returns PyTorch tensors as the output.\n","  * Result: train_encodings is a dictionary containing the tokenized and encoded representations of the training set, suitable for input to a BERT model. It includes keys like 'input_ids', 'attention_mask', etc.\n","\n","2. **test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True, max_length=128, return_tensors='pt'):** Similar to the training set, tokenizes and encodes the text data in the testing set using the BERT tokenizer.\n","  * Parameters: Similar to the parameters used for the training set.\n","  * Result: test_encodings is a dictionary containing the tokenized and encoded representations of the testing set, with the same structure as train_encodings.\n","\n","These lines of code prepare the text data in a format that can be fed into the BERT model. The tokenized sequences include information such as input IDs (token IDs representing each word), attention masks (indicating which parts of the sequences are padding), and other relevant details. This preprocessed data will be used as input during the training and evaluation of the sentiment analysis model."],"metadata":{"id":"cfPs_admx7Ib"}},{"cell_type":"code","source":["train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=128, return_tensors='pt')\n","test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True, max_length=128, return_tensors='pt')"],"metadata":{"id":"f8fjcDAhZ-He"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###convert label\n","In the bottom code snippet, the project is converting the labels from the training and testing sets to PyTorch tensors. Let's break down each line:\n","\n","1. **train_labels = torch.tensor(train_labels):** Converts the training labels from a NumPy array or any compatible data type to a PyTorch tensor.\n","Explanation: The original labels were numeric values obtained after using the LabelEncoder to convert categorical sentiment labels ('positive' and 'negative') into numerical labels (0 and 1). The conversion to PyTorch tensors is necessary because PyTorch models typically work with tensors.\n","\n","2. **test_labels = torch.tensor(test_labels):** Similar to the training set, converts the testing labels from a NumPy array or compatible data type to a PyTorch tensor. The same reasoning applies here—converting the testing labels to PyTorch tensors ensures consistency in the data type used for the labels throughout the model training and evaluation process.\n","\n","In summary, these lines of code ensure that the labels for both the training and testing sets are represented as PyTorch tensors. This is important because PyTorch tensors are the preferred data type for labels when working with PyTorch models, and it allows seamless integration of the labels with the PyTorch training pipeline."],"metadata":{"id":"S0QfrolE8WcC"}},{"cell_type":"code","source":["train_labels = torch.tensor(train_labels)\n","test_labels = torch.tensor(test_labels)"],"metadata":{"id":"bRlonuiBbWux"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Create TensorDataset\n","\n","In the bottom code snippet, the project is creating PyTorch TensorDataset objects for the training and testing sets. Let's break down each line:\n","\n","1. train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels): Combines the tokenized and encoded representations of the training set (train_encodings) with the corresponding labels (train_labels) into a PyTorch TensorDataset. The TensorDataset class is a PyTorch utility for creating a dataset by combining tensors along the first dimension. In this case, it combines the tokenized input IDs (train_encodings['input_ids']), attention masks (train_encodings['attention_mask']), and labels (train_labels) into a single dataset.\n","\n","2. test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels): Similar to the training set, combines the tokenized and encoded representations of the testing set (test_encodings) with the corresponding labels (test_labels) into a PyTorch TensorDataset. This line performs the same operation as the one for the training set but with the testing set.\n","\n","The resulting TensorDataset objects (train_dataset and test_dataset) provide an organized and convenient way to access and iterate over batches of data during training and evaluation. Each batch will contain input IDs, attention masks, and labels, making it easy to feed the data into a PyTorch model during the training process."],"metadata":{"id":"mcH6MCql9-cd"}},{"cell_type":"code","source":["train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n","test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)"],"metadata":{"id":"ANuccs-zddk1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### setting Dataloader\n","\n","In the bottom code snippet, the project is setting up PyTorch DataLoader objects for the training and testing datasets. Let's break down each line:\n","\n","1. **train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True):** Creates a PyTorch DataLoader for the training dataset (train_dataset).\n","  * Parameters:\n","    1. train_dataset: The PyTorch TensorDataset containing the tokenized input IDs, attention masks, and labels for the training set.\n","    2. batch_size=16: Specifies the number of samples in each batch during training. In this case, each batch will contain 16 samples.\n","    3. shuffle=True: Shuffles the training data at the beginning of each epoch. Shuffling is beneficial during training to ensure that the model doesn't learn patterns specific to the order of the data.\n","\n","2. **test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False):** Similar to the training set, creates a PyTorch DataLoader for the testing dataset (test_dataset).\n","  * Parameters:\n","    1. test_dataset: The PyTorch TensorDataset containing the tokenized input IDs, attention masks, and labels for the testing set.\n","    2. batch_size=16: Similar to the training set, each batch during testing will contain 16 samples.\n","    3. shuffle=False: No need to shuffle the testing data. Shuffling is typically done during training but not during testing or evaluation.\n","\n","The DataLoader is a PyTorch utility that provides an iterable over the dataset in batches. It is used during the training loop to feed batches of data to the model. The specified batch size controls how many samples are processed together in each iteration, and shuffling helps prevent the model from memorizing the order of the data.\n","\n","In summary, these lines of code set up PyTorch DataLoader objects for both the training and testing datasets, making it easier to iterate over batches of data during the training and evaluation phases of the machine learning model."],"metadata":{"id":"ODxQ5gEk_LUi"}},{"cell_type":"code","source":["train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"],"metadata":{"id":"SNSIItUcdl9y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###configuration parameters for train set\n","\n","in The bottom code snippet is setting up the optimizer for training the model and specifying the number of epochs for the training process. Let's break down each line:\n","\n","1. **optimizer = AdamW(model.parameters(), lr=5e-5):** Initializes the AdamW optimizer for updating the parameters of the model during training.\n","  * Parameters:\n","    1. model.parameters(): Specifies the parameters (weights and biases) of the model that the optimizer will update during training.\n","    2. lr=5e-5: Sets the learning rate for the optimizer. The learning rate controls the size of the step the optimizer takes during each parameter update.\n","  * Explanation: AdamW (Adam with weight decay) is a variant of the Adam optimizer that includes a weight decay term to help with regularization. It is commonly used for fine-tuning pre-trained models. The learning rate of 5e-5 is a commonly used value for fine-tuning BERT models.\n","\n","2. **epochs = 3:** Specifies the number of training epochs, where one epoch corresponds to one complete pass through the entire training dataset. During each epoch, the model goes through all batches of the training data, updates its parameters using backpropagation and the optimizer, and evaluates performance on the validation set if applicable. Setting the number of epochs helps control the duration and extent of the training process.\n","\n","In summary, these lines of code configure the training parameters for the machine learning model. The optimizer (AdamW) is set up to update the model parameters based on the training data, and the number of epochs determines how many times the entire training dataset will be used to train the model. Adjusting the learning rate and the number of epochs can impact the training process and the final performance of the model."],"metadata":{"id":"50tHU-DpAtQM"}},{"cell_type":"code","source":["optimizer = AdamW(model.parameters(), lr=5e-5)\n","epochs = 3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7nFWOE-gdwBH","executionInfo":{"status":"ok","timestamp":1703999259679,"user_tz":-210,"elapsed":762,"user":{"displayName":"Masoud Gaming","userId":"01529317132170938105"}},"outputId":"49dbac66-2528-49ef-ef46-9d4b40440bfe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["###configuration model for train\n","\n","the bottom code snippet, is responsible for determining the device on which the model will be trained and then moving the model to that device. Additionally, it sets the model to training mode. Let's break down each line:\n","\n","1. **device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'):** Determines the device (GPU or CPU) available for training. If a GPU is available (torch.cuda.is_available() is True), the model will be trained on the GPU; otherwise, it will use the CPU. PyTorch allows you to utilize GPUs for faster training if they are available. The torch.device function is used to create a device object, and the model will be moved to this device in the next line.\n","\n","2. **model.to(device):** Moves the entire model (including its parameters) to the specified device (GPU or CPU). This step is necessary to ensure that the model's computations are performed on the chosen device. PyTorch provides this flexibility to seamlessly move models between different devices.\n","\n","3. **model.train():**\n","Role: Sets the model to training mode. This is important because certain layers, such as dropout layers, behave differently during training and evaluation. During the training phase, the model needs to update its parameters based on the gradients computed during backpropagation. Setting the model to training mode activates these training-specific behaviors.\n","\n","In summary, this code segment ensures that the model is moved to the available GPU (if present) or CPU for training and is set to training mode. This device configuration is crucial for efficient training, and setting the model to training mode ensures the correct behavior of layers during the training phase."],"metadata":{"id":"Me7FA5dmCFDj"}},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","model.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"43wpeqRvdx9x","executionInfo":{"status":"ok","timestamp":1703999265636,"user_tz":-210,"elapsed":858,"user":{"displayName":"Masoud Gaming","userId":"01529317132170938105"}},"outputId":"219b26ed-7ace-44eb-fc9b-8ca185e948fc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["##Model training\n","\n","The bottom code snippet represents the training loop for the machine learning model. It iterates through the specified number of epochs and processes batches of data using the training DataLoader. Let's break down each part:\n","\n","1. **Outer Loop (for epoch in range(epochs):):** Iterates over the specified number of epochs (epochs), representing the number of times the entire training dataset is processed. The training loop repeats the inner loop for each epoch, allowing the model to learn from the entire training dataset multiple times.\n","\n","2. **Inner Loop (for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}'):):):** Iterates over batches of data from the training DataLoader (train_loader). Each batch contains a set of input IDs, attention masks, and labels for the model to process. The tqdm function is used to create a progress bar for better visualization of training progress.\n","  1. Data Preparation (input_ids, attention_mask, labels = batch): Unpacks the elements from the batch. The batch is a tuple containing input IDs, attention masks, and labels. This line unpacks these elements into separate variables for easier handling.\n","  2. Move Data to Device (input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)): Moves the batch data to the specified device (GPU or CPU). Ensures that the data is on the same device as the model for computation.\n","  3. Optimizer Step (optimizer.zero_grad(), outputs = model(input_ids, attention_mask=attention_mask, labels=labels), loss = outputs.loss, loss.backward(), optimizer.step()): Performs a single optimization step on the model using the current batch. Explanation:\n","    * optimizer.zero_grad(): Clears the gradients of all model parameters before computing gradients for the current batch.\n","    * outputs = model(...): Feeds the input data through the model and obtains the model's predictions and associated information.\n","    * loss = outputs.loss: Extracts the loss value from the model's output.\n","    * loss.backward(): Computes the gradients of the loss with respect to the model parameters using backpropagation.\n","    * optimizer.step(): Updates the model parameters using the computed gradients and the optimizer's update rule.\n","\n","This training loop is a fundamental part of the machine learning process, where the model learns to make better predictions by adjusting its parameters based on the training data. The outer loop controls the number of passes over the entire training dataset (epochs), and the inner loop processes the data in batches, updating the model parameters after each batch. The use of tqdm provides a progress bar for better monitoring during training."],"metadata":{"id":"wL9s8KfeCgrS"}},{"cell_type":"code","source":["for epoch in range(epochs):\n","    for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}'):\n","        input_ids, attention_mask, labels = batch\n","        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yj6gc_Twd8zx","executionInfo":{"status":"ok","timestamp":1704002051406,"user_tz":-210,"elapsed":1130421,"user":{"displayName":"Masoud Gaming","userId":"01529317132170938105"}},"outputId":"f3e332fc-c7fb-41d4-fcb1-47b1acc0d985"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 1/3: 100%|██████████| 2500/2500 [15:16<00:00,  2.73it/s]\n","Epoch 2/3: 100%|██████████| 2500/2500 [15:22<00:00,  2.71it/s]\n","Epoch 3/3: 100%|██████████| 2500/2500 [15:22<00:00,  2.71it/s]\n"]}]},{"cell_type":"markdown","source":["##Model Evaluation\n","\n","The bottom code snippet is part of the evaluation process for the machine learning model. It sets the model to evaluation mode, makes predictions on the test dataset using the provided DataLoader (test_loader), and collects the predicted labels. Let's break down each part:\n","\n","1. **model.eval():** Sets the model to evaluation mode. During evaluation, the model's behavior might differ from training. For example, layers like dropout layers may operate differently. Setting the model to evaluation mode ensures consistent evaluation behavior.\n","\n","2. **predictions = []:** Initializes an empty list to store the model's predictions. The model will generate predictions for each batch of the test dataset, and these predictions will be stored in the predictions list.\n","3. **with torch.no_grad()::** Temporarily disables gradient computation during the evaluation to save memory. Since evaluation doesn't involve parameter updates, it is more memory-efficient to disable gradient computation using torch.no_grad().\n","\n","3. **Loop Over Test DataLoader (for batch in tqdm(test_loader, desc='Evaluating'):):** Iterates over batches of the test dataset. Similar to the training loop, this loop processes batches of test data using the test_loader.\n","\n","4. **Data Preparation (input_ids, attention_mask, labels = batch):** Unpacks the elements from the batch. The batch is a tuple containing input IDs, attention masks, and labels. This line unpacks these elements into separate variables for easier handling.\n","\n","5. **Move Data to Device (input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)):** Moves the batch data to the specified device (GPU or CPU). Ensures that the data is on the same device as the model for computation.\n","\n","6. **Model Inference (outputs = model(input_ids, attention_mask=attention_mask), logits = outputs.logits):** Obtains model predictions on the test batch. The input data is fed through the model (model(input_ids, attention_mask=attention_mask)), and the raw logits (output before applying activation function) are obtained from outputs.logits.\n","\n","7. **Generate Predictions (predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())):** Extends the predictions list with the model's predicted labels for the current batch. The torch.argmax(logits, dim=1) extracts the class indices with the highest logits (predicted class for each sample). .cpu().numpy() is used to move the results to the CPU and convert them to a NumPy array before extending the predictions list.\n","\n","In summary, this code sets up the model for evaluation, iterates over batches in the test dataset, makes predictions, and collects these predictions in a list (predictions). This list can then be used to assess the model's performance on the test set, compare predictions with true labels, and generate evaluation metrics."],"metadata":{"id":"xXpennLoErzb"}},{"cell_type":"code","source":["model.eval()\n","predictions = []\n","\n","with torch.no_grad():\n","    for batch in tqdm(test_loader, desc='Evaluating'):\n","        input_ids, attention_mask, labels = batch\n","        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","        outputs = model(input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VTPySbQPZ1FM","executionInfo":{"status":"ok","timestamp":1704002187417,"user_tz":-210,"elapsed":81111,"user":{"displayName":"Masoud Gaming","userId":"01529317132170938105"}},"outputId":"945da5f3-338e-475a-b233-90dac7ecf636"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 625/625 [01:20<00:00,  7.72it/s]\n"]}]},{"cell_type":"markdown","source":["### Calculate accuracy and the classification report (precision, recall, F1-Score, support)\n","\n","The bottom code snippet evaluates the performance of the machine learning model on the test set and prints accuracy along with a detailed classification report. Let's break down each part:\n","\n","1. **accuracy = accuracy_score(test_labels.numpy(), predictions):** Computes the accuracy of the model's predictions on the test set. The accuracy_score function from scikit-learn is used to compare the true labels (test_labels.numpy()) with the predicted labels (predictions) and calculate the accuracy. The accuracy is the ratio of correctly predicted samples to the total number of samples.\n","\n","2. **print(f'Accuracy: {accuracy:.2f}'):** Prints the calculated accuracy. The accuracy is printed with two decimal places for better readability.\n","\n","3. **print(classification_report(test_labels.numpy(), predictions)):** Prints a detailed classification report. The classification_report function from scikit-learn generates a text report containing precision, recall, F1-score, and support for each class (positive and negative). It provides a more detailed understanding of the model's performance beyond just accuracy.\n","\n","In summary, these lines of code assess and report the model's performance on the test set. The accuracy gives a general measure of correct predictions, while the classification report provides more detailed metrics for each class, including precision, recall, and F1-score. This information is valuable for understanding how well the model is performing on specific aspects of the sentiment analysis task."],"metadata":{"id":"hSATATGjQCeU"}},{"cell_type":"code","source":["accuracy = accuracy_score(test_labels.numpy(), predictions)\n","print(f'Accuracy: {accuracy:.2f}')\n","print(classification_report(test_labels.numpy(), predictions))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PuupOqdFckmy","executionInfo":{"status":"ok","timestamp":1704002249319,"user_tz":-210,"elapsed":423,"user":{"displayName":"Masoud Gaming","userId":"01529317132170938105"}},"outputId":"136300c9-617d-40f6-cdf9-05ee8e50f3f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.89\n","              precision    recall  f1-score   support\n","\n","           0       0.87      0.91      0.89      4961\n","           1       0.91      0.87      0.89      5039\n","\n","    accuracy                           0.89     10000\n","   macro avg       0.89      0.89      0.89     10000\n","weighted avg       0.89      0.89      0.89     10000\n","\n"]}]},{"cell_type":"markdown","source":["## Samples for Check the model"],"metadata":{"id":"yVZBGbXIRa_R"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForSequenceClassification\n","import torch\n","\n","#input_text = \"This movie was amazing! The plot was captivating, and the acting was superb. I highly recommend it.\"\n","\n","#input_text = \"The movie was very stupid. It didn't help at all and had no content. The picture and sound quality was very low.\"\n","\n","input_text = \"The movie was very nice, but the ending was not good.\"\n","\n","# Tokenize and prepare the input for the model\n","input_ids = tokenizer.encode(input_text, return_tensors='pt')\n","attention_mask = tokenizer(input_text, return_tensors='pt', truncation=True, padding=True)['attention_mask']\n","\n","# Ensure both input and model are on the same device\n","input_ids = input_ids.to(model.device)\n","attention_mask = attention_mask.to(model.device)\n","\n","# Make the prediction\n","with torch.no_grad():\n","    logits = model(input_ids, attention_mask=attention_mask).logits\n","\n","# Get the predicted class (0 for negative, 1 for positive)\n","predicted_class = torch.argmax(logits).item()\n","\n","# Map the predicted class back to the original sentiment labels\n","predicted_sentiment = label_encoder.inverse_transform([predicted_class])[0]\n","\n","# Display the result\n","print(f\"Predicted Sentiment: {predicted_sentiment}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"efg4ZhFedD7M","executionInfo":{"status":"ok","timestamp":1704016149150,"user_tz":-210,"elapsed":324,"user":{"displayName":"Masoud Gaming","userId":"01529317132170938105"}},"outputId":"fb080f4e-73d7-4847-b32d-4bec64095ec7"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted Sentiment: negative\n"]}]}]}